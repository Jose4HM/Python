{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proyecto final\n",
    "#### Curso: Computación 2\n",
    "#### Docente: Ing. Renzo Bolívar Valdivia\n",
    "#### Integrantes:\n",
    "\n",
    "\n",
    "#### 20193270\t\tHancco Mamani Jose Antonio\n",
    "#### 20190790\t\tChávez Benavides Paul Berlín\n",
    "#### 20190798\t\tLipa Ayamamani Lesly Vanesa\n",
    "#### 20192293\t\tTrujillo Patana Maribel\n",
    "#### 20192297\t\tPeralta Tito Erika Miriam\n",
    "\n",
    "## Objetivos\n",
    " Análisis exploratorio de datos utilizando Web-scraping con pre-procesamiento de datos nivel básico y análisis exploratorio en python utilizando Pandas\n",
    " Machine learning- Aprendizaje Supervisado\n",
    " Machine learning- Aprendizaje No supervisado\n",
    "\n",
    "## Introducción:\n",
    " Este proyecto es parte del curso de computación, cuyo proyecto final es el siguiente a realizar, este proyecto tuvo con la participación de 5 integrantes, integrantes que han trabajado colaborativamente usando plataformas como Gitlab y Python 3 con el entorno de anaconda. Se desarrolló los temas Numpy, Matplotlib, Web-scraping y machine learning, poniendo a prueba todo lo aprendido en el semestre, demostrando y aplicando nuestros conocimientos mediante este proyecto.\n",
    " Usando el motor de búsqueda de google, se trabajó con páginas web tanto como para extraer datos o base de datos (Wikipedia y kaggle) como foros de consulta que nos han ayudado con algunos problemas que teníamos a lo largo de la realización\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web-scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Se empieza con la importación de las librerías, para el primer método se usara las 3 y para el segundo método solo se utiliza pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Método 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Para el primer método se utilizas las primeras dos librerías, a comparación de los métodos, este método es más complejo pero no está de más saberlo, por si el otro método no funcionase se tiene este.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Se coloca la variable tipo string asignada a una variable, luego se obtiene la información de esta página usando el BeautifulSoup con la funciona response.text, que extraerá toda la información tipo texto, es necesario indicar \"html.parser\" porque es una pagina"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url=\"https://es.wikipedia.org/wiki/Liga_1_2019_(Per%C3%BA)#Evoluci%C3%B3n_de_las_clasificaciones\"\n",
    "response=requests.get(url)\n",
    "soup=BeautifulSoup(response.text, 'html.parser')\n",
    "#print(soup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Usando soup.find buscamos los tipos \"table\" de la clase \"wikitable sortable\", estos datos se sacan de la inspección de la página, este paso es el importante. Terminamos todo el comando con tdoby, que es la etiqueta con la está documentada la tabla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table=soup.find('table',{'class':'wikitable sortable'}).tbody\n",
    "#print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Usando el find_all encontramos todos los datos que tienen la etiqueta \"tr\", son los datos pertenecientes a la tabla que vamos a extraer. Tendrán un nombre acompañado de \\n, por ello usamos la función replace y las quitamos, al final toda esa lista se guarda en un data frame. Así tenemos listo nuestro index de nuestro data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows=table.find_all('tr')\n",
    "#columns=[v.text for v in rows[0].find_all('th')]\n",
    "#print(len(rows))\n",
    "columna=[v.text.replace('\\n','') for v in rows[0].find_all('th')]\n",
    "#print(columns)\n",
    "\n",
    "df=pd.DataFrame(columns=columna)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Con la función for continuamos extrayendo los demás datos, desde 2, porque la primera línea es vacía y no lo registrara. Tendrán en el nombre \\n, y de nuevo lo reemplazamos con la función replace, por cada iteración que significa cada fila, la añadirá al data frame ya creado, una vez que se termine con toda la iteración, el data frame quedara terminado. Ya solo queda convertirlo en un archivo csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(2,len(rows)):\n",
    "    tds=rows[i].find_all('td')\n",
    "    \n",
    "    values=[td.text.replace('\\n', '') for td in tds]\n",
    "    #print(values)\n",
    "        \n",
    "    df=df.append(pd.Series(values,index=columna), ignore_index=True)\n",
    "df.to_csv(\"data/Csv's/Liga_10.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metodo 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Este método se descubrió a raíz del problema para extraer más tablas de la misma página con el método anterior, al parecer solo dejaba extraer esa tabla, por ello se buscó alternativas y la mejor y más rápida fue esta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Primero usamos la función de pandas \"read_html\", que lee el archivo html mediante el link, y lo transformara a una data frame donde los elementos son cuadros a extraer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tabla = pd.read_html('https://es.wikipedia.org/wiki/Liga_1_2019_(Per%C3%BA)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Con la función \"len\" vemos el tamaño del dataframe, en este caso este tamaño será la cantidad de tablas disponibles para la extracción"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(Tabla) #tablas disponibles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Solo seleccionamos unas, las que realmente queremos, es por ello que como todo dataframe, podemos acceder a cada uno de sus elementos usando los corchetes con un número que indica el orden del objeto a \"extraer”.\n",
    " Cuando se tiene asignada la variable (aunque puede también no asignarse), se convierte en un dataframe usando \"to_csv\" y eligiendo el donde lo guardaremos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1=Tabla[5]\n",
    "df_1.to_csv(\"data/Csv's/Liga_1.csv\",index=False)#guardamos en un archivo csv \n",
    "df_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2=Tabla[20]\n",
    "df_2.to_csv(\"data/Csv's/Liga_2.csv\",index=False)#guardamos en un archivo csv \n",
    "df_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_3=Tabla[19]\n",
    "df_3.to_csv(\"data/Csv's/Liga_3.csv\",index=False)#guardamos en un archivo csv \n",
    "df_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_4=Tabla[9]\n",
    "df_4.to_csv(\"data/Csv's/Liga_4.csv\",index=False)#guardamos en un archivo csv \n",
    "df_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_5=Tabla[18]\n",
    "df_5.to_csv(\"data/Csv's/Liga_5.csv\",index=False)#guardamos en un archivo csv \n",
    "df_5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Para tablas consecutivas, es mejor usar un for para que extraiga, es por ello que se usó un for junto con un range, donde va el intervalo de las tablas, se usa la misma constante de iteración para seleccionar el cuadro y se asignó la variable n como variable auxiliar para nombrar los csv's, variable empezada en 6 para no alterar la numeración de los archivos, y esta se irá incrementando en uno con cada repetición."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=6\n",
    "for i in range(29,33):\n",
    "    df_t=Tabla[i]\n",
    "    df_t.to_csv(\"data/Csv's/Liga_{}.csv\".format(n),index=False)#guardamos en un archivo csv \n",
    "    n+=1\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
